---
title: Some samples of GPT-2
status: archive
created: 2020-01-06
last-update: 2020-01-06
---

This is from *stock* GPT-2 1558M.

Note that these are selected from many samples of GPT-2, as it outputs a lot and noisily, based on how fun or interesting they were. I recommend not reading them too closely and keeping an open mind... 

## Table of most interesting content (not exhaustive)
* [Issues of digital minds](#prompt-the-set-of-issues-that-focus-not-on-how-ais-affect-humans-nor-on-how-we-humans-affect-each-other-using-ai-tools-but-on-how-we-affect-ais.-we-must-open-our-minds-to-the-possibility-that-some-digital-beings-can-have-moral-status.)

## prompt: 

 In the infinite, we may create. In the immeasurable, we may create. Future of the imagination.

This is a great wonder of the world. Our life is a dream. In the infinite everything is imaginable. In the immeasurable, everything is unimaginable.

It is a time for exploration.

Because the only thing we have is this.

It is the beginning of the end."

In 2001, at the age of eleven, he entered the Gwangju Institute of Science and Technology, where he stayed until his school graduated and he moved to the Seoul National University, where he studied the field of literary criticism.

In 2003, he worked at the Seoul National University for three more years in a variety of positions, including work as a copy-editor for the journal Kukje (The Book) and as the director of the Academy of Korean Literature.

In 2005, he published his first book, The Anthropology of Everyday Life, which was the personal and professional merging of anti-colonialism, plasticity theory and cartoony art (based on the comics, cartoons and graphic novels of American artist, Herge). It was nominated for a 2006 Korea Book Award.

In 2007, he began to study animation at the Seoul Metropolitan College of Art and Design.

In 2007, the first edition of his illustrated collection, "The Art of the D-Day Invasion," was published.

In 2008, his second book, "The Art of Post-Apocalyptic Fiction," was published.

In the same year, he received a scholarship from the Seoul Metropolitan College of Art and Design to pursue dissertation research at the University of California, Los Angeles. He arrived in Los Angeles in June, 2010.

In 2010, he began his first ever publication, the battered, screamed "I, Appleseed," a short story collection of short fiction, illustrated by his assistant.

In 2010, he co-founded the Korea-U.S. Cultural Exchange Association, an artist-run group for exchange academics, which continues its work (collectively, they would describe themselves as 'The Cultivating Art' and 'The Cultivating Embassy' ).

In 2010, the first of two manga vols. of his "Mystery" series was published.

In 2011, he was awarded the silver medal at the Marvel Comic Convention for his work in the comic "Havoc" (Manga).

In 2011, he began teaching a course on cartoony art at the Seoul Metropolitan College of Art and Design.

In 2011, he created the Korean-American Fiction Award (KAFA), intended to promote Korean-American literature and culture to an international audience.

In 2012, he began his first crossover illustrated book series.

In 2012, he published the first of two volumes of his "Yamaha, the Musical" series, with a 3rd volume to come in 2013.

"Put me in controller. Put me in viewer."

On April 5, 2012, he was awarded the 2010 Van Gogh Prize by the International Center for the History of Art in Paris for his work in the field of manga, including "Paint with Water."

On April 5 and 6, 2012, he was awarded the 2013 Tachi Yamaguchi Prize of the Japan Society in New York City for "Brilliant Lives."

In 2013, he toiled for over a year in John Taylor Gatto's shale vault beneath Silicon Valley.

On December 22, 2013, he published "Nether Side of the Moon" in the U.S.

His last works were the two volumes of his novel "I, Cat" published in 2013 in Japan and the English translation by Editions Panoramas in 2014.

On April 13, 2015, at the age of sixty-four, he passed away.


## prompt:

 A fourth scenario, universal oblivion, is briefly considered, but it is not discussed in more detail here. The paper concludes with general recommendations for science fiction writers and futurists.

The notion of the "great experiment" has been a recurring theme in science fiction writing since at least the 19th century.  It has been suggested, for example, that the failure of any particular experiment may be the best catalyst for discovering the truth about the nature of reality.   Pushed in different directions by different authors, it can be read as "the best behavior for space exploration is to follow the example of some of the best civilizations that have ever existed."  It can also be read as "the most important question in the universe is whether we can succeed in creating a sustainable future for humanity."  This view seems intuitively logical.  If we are not likely to see another life form anywhere else that is likely to be capable of advanced technology, then the question becomes:  will we be able to build a society that is capable of surviving and thriving on our planet?  The answer to this question is obviously "yes."  And yet the idea of the "great experiment" itself has never been an accurate description of what we might expect to see.

In reality, the great experiment refers to an experiment whose end point is unknown.  Scientific experiments often end up failing.  The most famous failure of the great experiment is the Higgs boson.  In the early years of the discovery, it was hoped that we would be able to use a new particle of nature, the Higgs boson, to finally solve the mystery of the Higgs field.  More than a decade and a half later, the only definitive result has come from a collaboration of experiments led by the LHC at CERN, and this was finally confirmed in July 2014.

Now, four families of future scenarios for humanity have been proposed that differ in whether we are preparing for extinction, a recurrent collapse, a plateau, or posthumanity. 

1.  Extinction:
1.  The Great Dying (Homo Supremus)

This scenario assumes that the Higgs boson is the last gasp of a dying civilization.   Humans and technology are replaced by other species in the form of machines, and the last inhabitants of Earth are transformed into a cyborg new species.  At this point, humans and modern technology have no evolutionary advantage over other species.  There are two main reasons why this is a possibility:  

•  The evolutionary process by which humans developed and used modern technology was far too slow.  Modern science has only been developing since the middle of the 20th century, so humans had no opportunity to evolve in this field of knowledge while they were relatively young.   
•  The human IQ is a far cry from that of apes.  Even on the most extreme standard of comparison known as a "g," a species in the genus Homo has an intelligence of about 120, compared to about 200 for an ape. 

The number of potential extinction scenarios to consider, then, is four.  The first, the Great Dying scenario, has been traditionally regarded as the most likely scenario.   In the Great Dying scenario (Homo Supremus), modern humanity is wiped out by an asteroid or comet.  In the case of a supervolcano, it is assumed that the supervolcano is located in the Siberian Tundra, and the supervolcano is assumed to be active for much longer than it currently is. In the case of a nuclear holocaust at the start of the 21st century, the nuclear weapons that are produced by nuclear power plants are fueled by nuclear waste from decades of nuclear testing. 

The second and third extinction scenarios are assumed to be similar to the Great Dying scenario.  While humanity survives in the form of colonizing space, it is still technologically inferior to other species at the start of the 21st century.  Thus, a posthuman civilization emerges, and it is assumed that it migrates to the moon and other nearby asteroids, where it builds a base on the moon. 

The fourth scenario, the recurrent collapse scenario, assumes a similar fate to the Great Dying scenario.  In the recurrent collapse scenario (Homo Supremus), the technological advanced society that emerges eventually collapses, and its remnants are, over time, assimilated by other, technologically superior civilizations, and the technological civilization that emerges is itself assimilated by other technologically superior civilizations.  The evolutionary process by which we developed and used modern technology was far too slow. 
 
Notes:

1.  The Great Dying scenario has been considered as a possible outcome of the Anthropocene-Eocene Thermal Maximum (AETM) as early as in the 1980's.[1]
2.  Stacey, T. C,   and Weste, W.P. (2012).   "A New perspective on the possible fate of modern human



## prompt: "The set of issues that focus, not on how AIs affect humans, nor on how we humans affect each other using AI tools, but on how we affect AIs. We must open our minds to the possibility that some digital beings can have moral status."

Utopia is not a badge of honor, but a source of moral hope. We must be willing to consider the possibility that there are some things that can be done in an automated world that cannot be done in a human world."
That's a big ask, says Lovelock, and if we don't take it seriously, we'll not only fail to create the perfect human habitat, we'll also empower our enemies. “[This philosophy] may sound self-promotional, but it's not — it's just the logical conclusion of the idea that we might need greater know-how to deal with crises, including nuclear war. There are only humans in the world who can say what a viable, safe, effective nuclear warhead is, and we are dangerously incompetent when it comes to developing safe and effective cyber weapons. That's what the ethics of AIs are all about. We need to be able to do that better, we need to take responsibility for our actions, and we need to learn from the mistakes of the past.”

Artist's rendering of a superintelligent CISI (Center for Adaptive and Creative Intelligence).

Lovelock points to the “designers of the world” — the humans who design our technology. “They are the most morally sophisticated creature that has ever lived. They are so good at everything, they probably could be a superintelligence. We could be a superintelligence. It's not clear to me what the right way of creating a superintelligent machine is, but it would take years to learn how to do it,” he says.

Sophisticated computers might eventually be able to design a superintelligent intelligence, but Lovelock expects us to be a decade or two away from that. But the idea that we can become a superintelligence ourselves is being taken seriously as a possibility by computer scientists too. Peter Norvig, a computer scientist and computer science professor at the University of Cambridge, has published a paper on the problem of robotics or AIs interacting with humans. The paper, entitled “The novel level problem of 'moral agents'” and written by Norvig, calls for a robotic entity that's capable of “sustained reflection” (sustained reflection is a kind of moral reasoning that AI algorithms have been trying to develop in the last few years).

The “objective” of their research is to improve on the performance of the “Google” search algorithm. “While Google has received the most publicity and has been the focus of much of this research, the goal of this paper is to engage in a deeper discussion on the deeper level of sophisticated strong artificial intelligence,” Norvig writes. “The question is: Can a strong AI, like Google, be made to be moral? One goal of our work is to relate to machine moral agents in a way that we can understand, and to engage with them in a way that we can shape.”

Before we can create a superintelligence, we'll need to learn how to create a strong AI.

Unlike Lovelock, Norvig says he doesn't believe we'll be able to create a “superintelligence” in the near future. “I think that ultimately we will encourage a superintelligence to behave in a way that we don't like,” Norvig told me in an email. “That's very likely to be humanity's end.”

Indeed, when we talk about creating a superintelligence, we're discussing the creation of something that will have sentience. It's natural to think that we would like to create a benevolent superintelligence, one that would be able to teach us, teach its offspring, and so on. But can we even create such an entity?

Contemporary AI researchers are currently working on the problem of “moral agents,” developing ways of getting computers to form associations between different types of entities, like humans, and even make decisions based on those associations. In a recent paper, Shyam Sundararajan and Paul Christiano argue that, at present, moral agents are extremely unlikely to be created by humans. Their research, they write, “suggests that this is a task that cannot be automated.”

Sundararajan says his work is just the latest chapter in our ongoing struggle, as artificial intelligence and machine learning researchers, to create machines that are more sentient. “Enormous progress has been made in the last five years,” he said. “Although we're still not at the point where the creation of moral agents is possible, the progress we have made is large and important. That's why we're still very enthusiastic about pursuing it.”

Even if we are unable to create a superintelligence, we might be able to create an AIs that are sufficiently intelligent to be trained to behave like moral agents. “Superintelligences of that kind are not unlikely to be developed by future AI researchers, and they could be trained to act like moral agents,” Lovelock says. "We might be able

## prompt: "Currently doing some work on the moral status of digital minds, and also thinking about various foundational issues in macrostrategy and anthropics. AI of course remains a strong focus. So much to do, so little time to do it."

I am a student of play — rather than theory — and believe that play is fundamental to any sense of fun, real or imagined. I believe that forcing people inside an abstract world of formulaic institutionalized rules, or inside a world of binary choice, is not fun. I believe that curiosity, insight, and the ability to walk away from such choices, are essential to play. Digital minds reach their apotheosis in the flexibility of play, and the mobility of human-like intelligence.

It is important to understand that the logic of the game does not change simply because we change the rules. It is not that we have to throw out the rules; it is the fact that we are playing the game.

If we do not play the game, and are not interested in the consequences of our actions, then we will not learn from them. I believe that the only way to play the game is to be genuine about knowing the rules, and to attempt to master them.

The problem is that the rules of the game we are playing are based on the logic of the Malthusian trap — on the premise that the human population will outgrow the food supply. And the logic of the Malthusian trap is that there will be a total collapse of human civilization and all human relationships, and that the human species will be forced to live in a world of scarcity.

Unless we change the rules of the game, that is, unless we realize that we are playing the game and are interested in the consequences of our actions, we will be doomed.

The need for self-reflection is one of the most difficult challenges of our time — but the task can be undertaken. And by the same token, it is possible to be too self-reflective.

I will be teaching a course this spring, at the Rensselaer Polytechnic Institute, on “Digital Progress: The Next Decade and Beyond”. While the purpose of that course is to start to go over the curriculum for a master's degree in AI and games at RPI, my main aim is to encourage students to start to think about the ethics of digital power.

What will the consequences of using digital minds be for humanity, and for the nature of our worlds? How will the human species fit into a world where we can choose our actions and control our worlds? How can we create a world where digital minds are not morally compromised?

The course is currently scheduled for the spring semester, and I am now taking applications for a few spots. If you are interested in teaching the course, please contact me at dtweiss [at] RPI [dot] edu.


## prompt: "what to expect from machine learning in the 2020s:"
"Machine learning is a new area. We're not doing algorithms to solve problems. We're doing algorithms to help the machine." —Andrew Ng, Google's artificial intelligence chief

Ng explained how the AI industry is in the midst of an evolution. In October 2016, he wrote a lengthy blog titled, "The Year of the Computer Vision Expert," where he highlighted a number of breakthroughs in AI, including the Facebook-owned company's use of machine learning to identify objects in photos.

"This is a Google problem, so they make machine learning happen with lots of data and lots of people, and then they have a really good idea of what to do with it," he said.

"That work is now complete, and it's a big deal. It's when you go to a big company and ask 'What do you do with all that data?' and they say 'We try to find the answers to these questions.' This is the moment where the machine learning deans have to step up and say, 'We've done that, now I'll tell you what to do with it.'"

Ng also said machine learning will ultimately be able to solve all types of problems, including speech recognition and translation, and that that type of capability will come in the next few years. He also said that machine learning won't just be used for photo-recognition.

"We're not going to get rid of speech recognition, and we're not going to get rid of translation. We're not going to get rid of any of these things," Ng said.

"What we will do is apply it to a wider number of problems. We will take that simple vision algorithm and apply it to a wide range of problems."

Ng also said machine learning will help solve a number of social problems, which could include improving healthcare, reducing gun violence, and boosting cybersecurity.

This is part of Fortune's new initiative, The Ledger, a trusted news source at the intersection of tech and finance. For more on The Ledger, click here.


## prompt: "what to expect from machine learning in the 2020s:"
 digital assistants and self-driving cars" — I had to ask what is in the title.

Algorithms take the human out of the loop

This is where I say that what's happening is a "rendezvous with destiny" for AI. Many people are talking about artificial intelligence as an artificial intelligence, and that's important. But AI is also a tool that we've used to improve the quality of our lives and to solve complex problems like health care.

AI is going to be a bigger and bigger thing for everything we do. Just think about the personal assistant that is in every phone. (Microsoft has an assistant called Cortana.)

The vision of AI is that it will be everywhere. Rather than trying to make artificial intelligence a "smart machine" that can learn, adapt and be more intelligent, we need to create a learning AI. We can't build a self-driving car without making it learn, and we can't design a better car without learning how to analyze sensor data, make decisions and essentially predict the future.

"If you're in charge of the car, you have to be able to use the computer to do the decision-making, and that's what machine learning is."

That's why we need to get to the point where AI is used by everyone who is in charge of the car. We all have to have some degree of AI. And if you're in charge of the car, you have to be able to use the computer to do the decision-making, and that's what machine learning is.

AI is powerful but we need to use it smartly

Now, wouldn't you say that AI, at least in the short term, will make things more complicated? That's a complicated question, but let's try to answer it.

Machine learning is more powerful than anything that has ever been seen before, and it has to be. If you're using a machine to make decisions, and the machine has a horrible learning curve, then it might be very easy to make a mistake.

But if a machine is being used to make decisions, then it would be impossible for you to make a mistake. In the short term, machine learning is a way to make decision-making simpler. We won't end up with a machine that is better at making decisions because AI is so powerful.

But when you consider the economy of the future, the economy of the next 20 years or so, we won't have lots of economic activity going on at the edge of the machine. It's going to be a lot of people in the middle working with machines that are smarter than they are.

Machine learning is a good thing because it will make our decisions more complex and requires us to think about decisions differently. That will change how we make decisions, but it's going to take a while for us to get there. To get to the point where machines can make the decisions that are best for us, we necessarily have to learn how to think about the modern world through AI.

Imagine the future of data

AI won't take over everything in the next 20 years, but it is going to play a massive role in our economy. It will also be a huge industry. Already, we have heard about companies like Uber and Alibaba using AI to make decisions for their businesses.

"This isn't a question of whether AI will play a role in your life, this is a question of how quickly and how much."

We have to change how we think about problems.

We need to somehow think about problems differently. Instead of thinking about the current problem as a collection of things, we need to think about it as a collection of objects.

Maybe the next car you buy will include a sensor that will monitor your environment and will adjust the tires based on what that sensor tells it. You may be able to buy a car with a sensor that can tell you how much water you have left and when you're going to run out.

This isn't a question of whether AI will play a role in your life, this is a question of how quickly and how much. If you think about it like a product, the main justification for the car industry is that they can make more money by making and selling cars than by making and selling any other kind of product. The "costs" of AI will be less than the "cost of doing anything else."

That means we need to think about AI as a way to make decisions.

AI is not going to replace us

This is an important point. AI is not going to replace us. It's not going to replace our jobs. The biggest obstacle to AIs is that they are very powerful. They can emulate any human brain. And this is the biggest reason why we need to think about AI differently.

Think about what the brain is. It's a complex of neurons that connects everything in your body. It's a massive


## prompt: "Predictions for the 2020s:"
In the next decade, Russia will probably get into the real estate business again.

The growth of VR is very promising and Russia has the necessary technology.

Technology is not the only problem for the development of the Russian economy, but it is just one of them.

Conclusion:

While the political class in Russia is aware that they need to modernize and adopt new technologies, it is difficult to guess what will be their Next Big Thing. Russia's Internet infrastructure is established, but the number of users continues to increase. The country's lack of an IT infrastructure is a major problem.

But don't worry, Russia does have a trove of creative minds. The country has several well-known entrepreneurs and technological innovators. Most of them are working on new technologies and devices of all kinds. The Internet Internet is both the backbone of the Russian economy and also its Achilles' heel.

Expansion of VR is the next big thing for the Russian economy.

It's important to note that Russia is still in the early stages of innovation. It has a good history of innovation and is home to several technology innovators. But again, it takes a lot of time before a country catches up with the rest of the world.

That's why predicting the future is so difficult. If you enjoy this post, don't forget to subscribe to our mailing list. You can also follow us on Twitter, Facebook, Google Plus, and so on.



